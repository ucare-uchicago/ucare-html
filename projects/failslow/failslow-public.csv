TYPE OF LIMPING HARDWARE,MONTH AND YEAR,LIMPING SYMPTOMS,ROOT CAUSES,HIGH-LEVEL IMPACTS,TIME TO DETECT,TIME TO RESOLVE,FIXED or REPLACED?,EXTRA NOTES
PROC,October - November 2015,"The Company 5 Observability stack ingests ~2.8 billion metrics a minute.  We collect metrics on a minute interval.  The metrics processing is latency sensitive since users define alerts which are ran on the most recent data.  In October we started noticing that the metric write time was increasing, and sometimes taking more than a minute to process all the previous mintues metrics.  We had just recently added capacity to our database, but put in a request to add more since our write latency is proportional to the number of writes per minute and the number of machines in the cluster.  After adding more nodes to the database cluster we observed the write time actually increased.  It took a month to diagnose the problem, the BIOS was incorrectly downclocking the CPUs of the new machines being added to the database cluster.  These machines were limping along but were assigned the same number of shards and therefore requests of correctly clocked machines.  ",BIOS set the CPU frequency to a much lower speed than the machines were capable of. (GREAT GREAT!!) ,"Late Metric Writes resulted in false alerts, often times alerts are written to detect missing metrics since this is usually a signal that something is wrong.  When metrics are processed late these alerts incorrectly fire.  ",1 month,??,FIXED: Updated bios,as soon as we updated the bios to appropriately set the CPU frequency the latency of writes returned to expected times
PROC,,CPU was throttled to about 50%,"A particular sku of machines in the data center had 4 machines sharing 2 power supplies, however when one power supply failed, there was not enough power to run all 4 machines at normal capacity, so it throttled the CPU on each machine by 50%   (GREAT GREAT!)","Low success rate on an indexing service, because the machines could not keep up with the number of requests","This took a while, a couple days, to resolve because we had no visibility into power supply health.  ",??,,
SSDD,,"This was in a Hadoop cluster and resulted in data loss, with that many machines failing regularly, a factor of three replication wasn't enough.  Also the servers would not shut down properly because that required a successful write to disk to do so.  So we saw lower success rates because broken machines with failed SSDs would try to serve traffic, and couldn't shut themselves down.   ",The controller on this brand of SSDs was bad and would stop responding about 3-5% of the drives would be failing each week (GOOD!),"lost data, performance degradation visible as failing jobs. ",,??,Replaced all SSDs of this SKU,
DISK,,"Hadoop jobs would fail based on whether a certain seciont of a jar file was accessed by the Hadoop task.  The taks that accesssed a corrupt portion of the disk where the jar file was mapped would fail.  This originally surfaces as JVM fault accessing memory error, but was actually a disk failure.  Retrying the job on a different machine without the corrupted disk would succeed.  Eventually the disk monitor decided that this particular disk was prone to frequent faults, so offlined the disk and the faults went away","Unknown (OKAY, not really limpware?)","Hadoop jobs had to be retried multiple times to run successfully, this was handled in software.  This case is interesting because it presented as a JVM error when failing.  so it was difficult to debug or ascertain what was actually happening.  ",,??,,
NET,2016,"Poor performance via our firewall appliance (a SonicWALL SM9200).  Individual TCP (and UDP) flows were limited to about 150 Mbps;
Asymmetric loss going on that
measured at less than 1%.","Disabled optimization settings;
Accumulated CRC errors",perturbed TCP flows,2-3 months,??,FIXED: firmware update,[More detailed stories in email]
NET,2000,"MAC learning not being very responsive;
Special type of traffic (ie. multicast) not working well.",traffic floods,,,??,,
NET,2016,"High (10%ish)
packet loss rate",clogged air filter,"optics in the switch start to failing due to high temperature, and problem return.",2 days,??,FIXED: replaced filters,
NET,2005,High packet loss rate,bad copper cables and pinched fiber optics,Inter-cluster communication issues.,"1-2 days, multiple occurences (2)",??,REPLACED transceivers/cables or FIXED pinched fibers,
DISK,2010,Some disks that went into “read after write” mode when outside of their normal operating parameters.,disk get too cold,Poor performance,~ months,??,REPLACED: drives,
DISK,2013-06-24,"RAID controller on boss node locked up. After restart, its working, but there are (more than) occasional timeouts and retries.",,"boss appears to be running slowly with occasional
pauses. This might manifest as swapins failing or other control
functions failing.",2013-06-26,??,REPLACED,
MEM,03/1976,Machine would branch to location that was not a valid address,"Machine ran fine in Minnesota but would only run for a few minutes in Nat'l Labs Y, memory system was not delivered with any memory protection (CRC/Parity/etc.).  Cosmic particle event incidence is many times higher at Nat'l Labs Y Altitude (7500 ft) and single/multi bit upsets were frequent enough to make the machine non functional.",Memory system was shipped back and reworked. ,5 months.,??,Hardware memory protection (integrated circuits) was developed and added to the memory controller boards.,"This was the first Cray system ever delivered and ran only a
few minutes before the first upset at Nat'l Labs Y.  The machine ran reliably for hours in Minnesota."
PROC,05/2002,Nodes would branch to an address that doesn’t exist and hang the node,"Unprotected bus on cpu card, cosmic events would cause address bus to corrupt addresses","Nodes would hang, progress indicators on applications would kill job, node would be taken out of service, job would restart on different nodes, large long running jobs would be effected about once a day","Months to diagnose, finally took node to Nat'l Labs Y Neutron Science Center and put machine in neutron beam which simulates many times the cosmic particle bombardment than sea level and many times low space orbit even.  Statistically showed that node hangs occurred at about the incident rate of cosmic particle upsets for the area involved.  Finally an analysis by the vendor discovered an unprotected bus, once the beam/statistical analysis had been done.",??,"Problem was never fixed, it was decided we would live with the problem due to the fact that it only affected large long running jobs about once per day and those jobs have restart capability.",Vendor provided financial assistance to compensate
PROC,12/2005,System would corrupt data during calculations,"Binning issue, at advertised clock rate, at advertised temperature range, at advertised voltage range, CPU would corrupt calculation.  It was never determined exactly in what way the CPU was failing other than a yield/binning issue.",System would not calculate correct answers reliably until after resolution was applied.,"Time to diagnose was months due to not reliably able to reproduce, needed a statistical approach to characterize the issue and one to determine if resolution worked.",??,Fix involved turning down clock slightly and putting a software monitor on processor temperature and voltage and killing the node if voltage/temperature got close to the edge of the binned values.  ,Vendor provided financial assistance to compensate
PROC,7/2008,"System would corrupt calculation, only on certain applications",Capacitor on power control logic on motherboard under certain loads would not provide adequate power/voltage to CPU which would put the processor out of spec for bin for less than a millisecond,System would not provide reliable calculations for certain applications until resolution applied,Time to diagnosis was months due to the fact that the problem could not be reliably reproduced thus it turned into a statistical problem for characterizing and for determining if resolution fix worked.,??,Small capacitor was added to each motherboard on site for thousands of nodes.,Vendor provided financial assistance to compensate
SSDD,6/2015,Flash device would stop responding for seconds and then recover,Microcode on flash device ,Flash layer was not put into production for several months until fix was available.  Users had to bypass flash layer and use parallel file system directly making for longer runtime/slower workflow.,1 month,??,Microcode patch,Vendor provided financial assistance to compensate
MEM,7/2016,Available memory size would decrease after some run time,"Binning problem, software filter was developed and run to return failing processor packages",Problem discovered and fixed during acceptance of machine,7 Weeks,??,software filter run to screen and return failing units,
DISK,2008,More frequent rebuilds than normal on raid/erasure,Temporary disk bit rot,"Performance issues with file systems, had to reformat disk file systems",8 Months,??,Added linear encoding on disk to eliminate temporary issues,
NET,5/2016,Interconnect errors causing lots of jobs to fail and restart,Binning problem with electronics in active cables/fibers,Very low productivity on machine(s),11 weeks,??,Software filter build and run to find bad cables/fibers and replace,Replaced over half the cables of the system(s)
PROC,9/2016,"1 application exhibits behavior of global communications every 1 millisecond, this behavior was causing an entire rack power supply to go unstable and dropping power to various parts of the rack.",Not enough capacitance in rack based power supply,Can not run the challenging application on this machine,2.5 Months to figure out and design power supply mod and validate its use and safety,??,Power supply mod adding capacitance to rack power supply,
NET,5/2004,"System hangs, poor performance, Link errors",Nic Design didn’t work,Very low productivity on machine, 2 Months,??,After 2 re-spins of the NIC Chip Vendor canceled the product,Had go out and purchase a different vendors NIC
PROC,7/2016,CPU would get hot and Thermal Throttle,Vendor cooling design was not sufficient at Altitude 7500’,Variability in system performance,Not Resolved,??,,"Tried lowering water and air temperature. Set Fans at full speed, it helped but didn’t resolve"
PROC,7/2012,Node would shut down or thermal throttle,Fan firmware would not react quick enough for a CPU intensive MPI job. CPU would get hot and shut down.,Failed jobs.,3 Weeks,??,Set fans to always run at high speed not variable.,
NET,7/2013,HSN would get errors or NIC would fail,Motherboard design. The NIC was solder on the motherboard behind CPU and memory.,MPI jobs failing and network errors. Excessive motherboards RMAed,Not resolved,??,,"Tried lowering air temperature, taking off doors, cold aisle containment. Nothing helped."
NET,4/2014,"The Network would throttle, killing job performance and not coming out of it till a system reboot",Incorrect parameter set in xtnird.ini,Jobs were running slow,2 Weeks,??,Parameter change.,
NET,7/2010,Network would deadlock because of congestion. We saw silent data corruption with an app.,Network routing algorithm was doing bad things,"Slow jobs, incorrect data", 10 Weeks,??,New routing algorithm,
NET,5/2007,The Physical implementation didn't match design specs. Network not preforming.,starving a distant corner of the chip from electrons,Jobs were running slow or failing.,4 Months,??,Changed all the NICs to a different vendor,Very costly for the vendor
NET,June -Dec 16,HSN cable degrading and dropping Lanes or failing,Bad VSCEL in laser,"Links in HSN degrade and switch to switch links down, degrade cluster performane",5 months,??,"Replaced 100's of cable, Run a script to identfy failing cables",
NET,2013,"A bad batch of optical transceivers that experienced a high error rate, collapsing throughput from 7 Gbps to just 2 Kbps",Failing transceivers,"As the failure was not isolated, the affected cluster ceased to work.",,??,,
DISK,2013,"a set of disk volumes incurred a wait time as high as 103 seconds, uncorrected for 50 days, affecting the overall I/O performance",slow disks,overall I/O performance degrade,,??,,
DISK,2011-05-18 0:21,File servers abandon access to controller and LUN for the duration of 50 days until they are rebooted.,"frequent disk issues (e.g., I/O timeouts) cause the controller performed 71 “LUN resets”","At LUN reset time, the controller delayed responses to incoming I/O requests for up to 103 s, causing three of the file servers to timeout their outstanding I/Os and refuse further access to the affected LUNs, even after affected controller and LUNs resetted and back operational.",50 days,??,Reboot the three file server.,
DISK,"2011-05-17 20:30
2011-06-20 18:30
2011-09-08 02:00
2012-01-16 19:30","Long I/O wait time jitter occurring every few seconds, adding considerable jitter to I/O operations.","storage controller finds errors, usually I/O and “stuck link” errors on many disks within a single disk drawer.",Frequent IO jitter,4-52 days,??,"Operators resolve these errors by forcibly failing every disk in the drawer, rebooting the drawer, then reinserting all the disks into their respective arrays.",
DISK,"2011-06-18 08:00
2011-08-18 20:00
2011-09-25 04:00
2012-04-19 12:00
2012-06-25 16:00","A single LUN exhibits considerable I/O wait time (await) for as little as a few hours, or as long as many days.",,long I/O wait time (await),79 hour - 11 days,??,,
SSDD,Middle 2016,"Unrecovered read
sector error",Buggy firmware.,N/A (add more details),6 months,??,FIXED: Update the firmware.,
SSDD,2012,"SSD sometimes disappears from system, and later reappears.","SSD was doing some
internal metadata writes and that triggered certain hardware assertion
condition to fail",N/A (add more details),6 months,??,"FIXED: Improvements of NAND
quality and FTL",
DISK,2013,Hard drives perform poorly.,"A fan in a compute
node stopped working. Then, to compensate this failing fan, fans at other
compute nodes started to operate at their maximal speed. As a result, these
fans made lots of noise and vibration.",User performance degraded. (add more detail?),N/A,??,N/A,
NET,April 2013,Switch is responding slowly. Not able to login,"Lots of VLANs were created at this swtich and it was busy processing LACP, leading to a very high CPU load.",Switch is responding slowly. Not able to login,1 day,??,Reboot the switch and delete unwanted VLANs,
PROC,August 2015,Slow fan,One fan was spinning slowly below toleratable speed,Generated a system error/warning message,1 day,??,Replaced the slow fan,
NET,August 2016,very poor 10 Gb/s throughput,NIC vendor driver bug,extremely slow backup jobs,2 days,??,workaround to disable TCP offload,happened at multiple deployment sites
NET,March 2017,poor 10 Gb/s throughput,switch environment does not support jumbo frames ,failing backup application jobs,3 days,??,MTU size reconfigured to 1500 bytes,
PROC,,"The power strip's firmware gets
in a weird state, and whole racks failing off the power control",,,3 days,??,"sometimes only requires a reset of controller, sometimes has to be completely removed from power (breaker on/off), sometimes need a firmware re-flash, and in rare instances needs to be completely replaced.",
NET,,Cluster can not use infiniband although the IB card still detected by hardware,Newer kernels in Linux changed drivers that required a firmware update on the IB NIC. HP does dot release firmware update yet.,HP cluster stuck with an Ethernet-only,1 week,??,,
NET,,"Serial-over-LAN does not work with the standard ""ipmitool sol activate""... But, if we went to OpenIPMI's ipmiconsole command, then it would work.",The IPMI implementation is not consistent with some standard,,2 days,??,,
DISK,,"Performance of installed drives, WD Blue, were very sporadic. A node would experience 100 kB/s r/w speeds when in it's rack, but would do just fine @ 100MB/s when tested on office.","Faulty chassis fans in surrounding nodes, caused such strong vibrations to move the desktop drives into recovery mode.",Slow Disk,4 months,??,"Disassemble 1024 nodes, add vibration dampers to each of 8 harddrive screws, replace ~10% system fans in all nodes",
MEM,,The Nat'l Labs X Cluster blades had a lot of memory failures.,"memory failures are reported for pairs of memory stick, although only one is faulty.",difficult to tell which memory stick is faulty,3 months,??,,
MEM,,Faulty memory still pass when tested at 600Mhz,,33% of all memory from Nat'l Labs X Cluster had to be replaced,3 months (fixed during the same three months as issue NMC7),??,33% of all memory from Nat'l Labs X Cluster had to be removed and replaced with scavanged memory from bad blades,
PROC,,,CPU heatsinks did not in physical contact with the CPUs,Many nodes get overheating,1 week,??,,
DISK,July 2016,About 10% IO requests timeout,"Buggy, it started many duplicate data scan tasks.","User performance degraded, due to the bandwidths of some disks are occupied by the backgroud tasks.",2 weeks,??,"FIXED: on line, change backgroud task interval from 12 hours to 48 hours. Fixed the bug.",Software Coupling
PROC,May 2016,"CPU of the node is exhausted, all request became slow",Thrid party processes use too much CPU,Some VMs went into blue screen of death because of some IO requests hang.,2 hours,??,reduce the requests of the third party system,
DISK,January 2016,All disk connect to the raid card became slow,Single bad disk exhaust RAID card resources,"Almost all IO requests to the storage cluster timeout, which make all VMs go into blue screen of death.",2 hours,??,replace the disk,Subhealthy node
DISK,June 2016,"IO request message queue usage rate is gradually increased, eventually full",Disk limping ware,"Before the message queue is full (several hours), some IO requests slow and make some VMs go into blue screen of death",6 hours,??,"When message queue full, the process restart itself,and empty the message queue. Replace the disk later.",limping at idler time
NET,January 2016,"40% of big packages lost, no small package lost",NIC limping ware,Some VMs went into blue screen of death because of some IO requests hang.,1 hour,??,"Switch NIC from active to standby, and replace the failure NIC.",Subhealthy node
NET,June 2016,"~5% package lost, network traffics fall from 1~2MB to about 100KB.",NIC driver bug,Many VMs went into blue screen of death because of many IO requests hang.,1 hour,??,Reboot OS,Subhealthy node
NET,June 2016,Network delay up to hundreds of milliseconds,Network cable loose,"Storage cluster abnormal, all IO requests failue.",1 hour,??,Isolate the node and reboot all the other nodes.,Subhealthy node
NET,February 2016,30%~50% wrong packages,NIC limping ware,Some VMs went into blue screen of death because of some IO requests hang.,2 hours,??,Isolate the node,Subhealthy node
DISK,January 2016,"The disk is removed from the storage pool when it's status is bad, and add to the storage pool when good",Disk limping ware,"Clients can't mount the volume of this disk, and VMs went into blue screen of death",1 hour,??,replace the disk and reboot the node,
DISK,June 2016,"The delay of the IO request to this disk became tens of milliseconds, even hundreds of milliseconds",Disk limping ware,"Delete files tasks, which run as backgroud tasks, became very slow, and the space of storage pool is full, which make the storage cluster readonly",1 day,??,"replace the disk, and the speed of delete file tasks became faster",The priority of user IO request is higher than backgroud task
PROC,,Performance degrade,Overheating,Performance degrade,,??,,
NET,,1 slow infiniband nic cause entire cluster slow,1 slow infiniband,entire cluster slow,1 week,??,"Run at 12 nodes, takes out 2 nodes at a time to narrow down which node is slow. Replace/take out the node.",
PROC,,Machine run on slow speed than it capables,faulty sensor in motherborad report faulty value to OS,"OS to always run on energy saving mode, hence flaky performance.",1 week,??,Replace motherboard,
NET,,Network performance decrease to half,Dynamic routing algorithm on stock driver/firmware does not work as what promised.,Network performance decrease to half,,??,"Staff hack kernel to do ping between
switches to reveal the root cause. No fix from vendor (mellanox) yet.",
PROC,,Application does not run at highes speed setup,"Every time after upgrade, application go back to default setting,
losing all tuned high-performance setting.",Application slower than it should.,,,Reconfigure after every update.,
DISK,,Midway2 cluster slow,Hickup on Midway1 GPFS cluster can disturb Midway2,User cannot open/edit files,4 week,,reconfigure GPFS tuning parameter,
MEM,2016,Node becomes unavailable under load,"One pin on an ECC DIMM was bad, so 100% of DRAM reads failed, and were transparently corrected by the DRAM controller",Repeated failovers + alerts in production,< 1 day(?),,Replaced hardware; Modified failover algorithms to handle this,
SSDD,2015/6,Repeated SSD offline / online events.,"Data center contained a convenient table for staging.  Technician moved office chair from its original location to adjancent to storage cluster.  Sysadmin liked to rock in chair, and repeatedly popped hotplug drives out of chassis.",Performance SLA violations,Hours,,"FIXED: Moved chair, added SAS device sense status monitoring in later software update ",
NET,2015/16,Billions of SAS errors simultaneously reported by all the drives in a cluster,"Unkown (bad cable? RFI?)  A technician was working on another machine in the rack during the event, but attempts to reproduce the issue have failed (max 5 errors injected across multiple torture tests)",5 minutes of downtime,Unresolved,,Replaced cable,
MEM,2015,"NVRAM power status incorrect, allowing software to incorrectly arm NVRAM",Pullup resistor attached to wrong power bus,Theoretical widespread data corruption (caught in prototype build),Weeks,,FIXED,
SSDD,2015,"Elevated bit errors rates reading from flash, leading to performance issues and excessive page rebuilds.","Update to FPGA toolchain changed default pin voltages, leading to bitfilps in the data path between NAND and the FTL.",None,Days-weeks,,Fixed: Updated FPGA bitfile,
SSDD,After 2006,"The PLX 27895 PCIe switch was widely deployed in content creation (video) industries, especially by NVIDIA.  Storage HBA vendors started using it.  After it was shipped by multiple vendors, people discovered that it flipped bits at an increaing rate as the hardware aged.  The vendor only certified this part for video card use, not storage.","Inadequate solder pad allowed alpha radiation in at an increaing rate over time.  NVIDIA's graphics stack masks such errors, so their customers never noticed.",Data corruption across multiple storage vendors / product lines,??,,Replaced,
PROC,2016,"Once-reliable hardware started hanging without logging errors.  Only one model was affected, and they all started hitting the issue simultaneously","An inadequate capacitor causes voltage droop when multiple cores transition from parked to turbo-boost simultaneously.  New user-space scedulers made this more likely, and a new bios enabled turbo-boost on machines where doing so did not measurably improve performance.  Independent testing of the updated bios and updated software did not reproduce the issue",Repeated failovers + test failures in test cluster.,Weeks,,FIXED: Disabled turbo-boost for impacted board revision,
PROC,Confidential,Confidential,Confidential,"Worst case: delay rollout of version N+1, if we skipped validation, would lead to SLA violations, more frequent replacement of production systems",Confidential,,Generally Fixed: Confidential,
SSDD,2016-2018,,,"Misc FTL (firmware) issues: (1) < 5x write amplification for model A, > 600x for model B.  (2) Infinite write amplification for certain workloads, premature wear leading to hard-to-diagnose bricks.  (3) Certain sequences of I/O requests brick drive at power loss, (4) Certain SSDs go unavailable if Linux maintains a write queue that is too large, so tar -zxvf causes the machine to lose availabilty -- solution switch back from deadline to CFQ scheduler)",,,,
MEM,Ongoing,Machine Check Exceptions cause Linux to lock all the cores.,Bad DRAM page,Performance,-,,FIXED: Implement + publish kernel patch to mitigate this (and also mark one page bad at a time),
MEM,Ongoing,Periodic (infrequent) latency SLA violations,"SAS command path uses an SRAM, which flips bits, causing the HBA to reboot.","SAS failover, causing latency blips, and alerting support personnel",Ongoing,,Ignored,
MEM,Ongoing,FPGA exhibits undefined behavior (generally fail-stop) due to SRAM corruption,FPGA bitfile (circuit description) stored in SRAM,"Node failover due to ""failed"" disk.  RAID rebuild away from node",Ongoing,,"FIXED: Reduced performance impact of future occurrances, harden software against data corruption",
PROC,2015-16,"With probability 1/N certificates fail to validate, but only for one newly introduced protocol.","One bit is stuck on in one of the crypto accellerator registers on one core of a CPU.  The CPU is supposed to raise interrupts instead of providing wrong answers, but fails to do so for this instruction.  The problematic core is not core 0, so the Linux kernel deterministically gets lucky and successfully invokes the instruction on a working core.","Initiating secure connections to the cluster was unreliable, delaying roll out of new feature in production",?,,Replaced,
MEM,?,"Admin + Board management commands hang, preventing failover.  Have to power-cycle backplane network to recover.","Flux contamination on unknown number of printed circuit boards leads to flakey EEPROM reads+ lack of ""In circuit testing"" + software update causes repeated read of EEPROM at runtime",Correlated failure of redundant network interconnects,"Physical travel to multiple sites required to debug.  (Flux contamination can be temperature + humidity dependent).  Not clear if it is resolved or not (hit 8 occurances, and factory cannot produce list of impacted boards)",,"REPLACED: Impacted hardware  FIXED: reduced stress on EEPRORM path, improved manufacturing processes",
PROC,2016,"One time out of 1000, the machine fails to boot.",Non-grounded serial-port injects garbage keystrokes into bios boot prompt,Manufaturing delay,Days-weeks,,"FIXED: ground port in next hardware build, disable BIOS prompt",
PROC,Ongoing,"In ""dark"" environment (= no admin consoles allowed) two hours from nearest technician, machines pause at grub prompt at reboot.",Unknown,Two hour (minimum) outage at reboot,Unresolved,,Replacing with known-good machine did not help.  Returned box works fine on our end.,